{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f96fce36-8284-42d5-960d-bc46f367e39b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: nltk in c:\\programdata\\anaconda3\\lib\\site-packages (3.8.1)\n",
      "Requirement already satisfied: click in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk) (2023.10.3)\n",
      "Requirement already satisfied: tqdm in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk) (4.65.0)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2605a0c6-2778-4041-88af-06c7bdfadec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c7cbd9b-1c73-4c2c-b62a-93606bf6c4a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "   from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "99fd4c5f-6464-402e-a0d4-c0c9f5f97164",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_files\n",
    "movie_data = load_files(r\"C:\\Users\\User\\Downloads\\review_polarity\\txt_sentoken\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "88be479e-83de-4179-8556-38cdbd6bb046",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000 2000\n"
     ]
    }
   ],
   "source": [
    "x, y = movie_data.data, movie_data.target\n",
    "print(len(x), len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1a24e590-be95-4b1c-87c0-7ca4ca05732d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_data.target[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "92e30f2a-f00c-47a5-be9f-2e6ab2dc08f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "lst = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "10f2e0ff-e9d0-457a-9d2a-d3b6cfd693de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "stemmer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "183216dd-7b7b-44d6-9307-bc41f45bf0bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " import nltk\n",
    " nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "57e80b94-fd80-4937-972d-e27110f99a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "for sen in range(0, len(x)):\n",
    "    txt = re.sub(r'\\W', ' ', str(x[sen]))   \n",
    "    txt = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', txt)\n",
    "    txt = re.sub(r'^[a-zA-Z]\\s+', ' ', txt)\n",
    "    txt = re.sub(r'\\s+', ' ', txt)\n",
    "    txt = txt.lower()\n",
    "    txt = txt.split()\n",
    "    txt = [stemmer.lemmatize(word) for word in txt]\n",
    "    txt = ' '.join(txt)    \n",
    "# add the final text to the list\n",
    "    lst.append(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "51fe8270-195d-454a-93d0-9929470e4280",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "  \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "converter = TfidfVectorizer(stop_words=stopwords.words('english'))\n",
    "x = converter.fit_transform(lst).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3320030c-e6e6-45f9-85a1-ed65d9a4efae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0d5e96b7-9dcc-4cd9-972c-492cc352947d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear SVC: 0.8450\n",
      "Logistic Regression: 0.8200\n",
      "Random Forest: 0.7950\n",
      "Naive Bayes: 0.7925\n",
      "Decision Tree: 0.6775\n",
      "KNN: 0.6600\n",
      "\n",
      "Best Model: Linear SVC | Accuracy: 0.845\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "models = {\n",
    "    \"Naive Bayes\": MultinomialNB(),\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=1000, random_state=0),\n",
    "    \"Linear SVC\": LinearSVC(random_state=0),\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators=200, random_state=0),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(random_state=0),\n",
    "    \"KNN\": KNeighborsClassifier(n_neighbors=5)\n",
    "}\n",
    "\n",
    "results = {}\n",
    "for name, model in models.items():\n",
    "    model.fit(x_train, y_train)\n",
    "    y_pred = model.predict(x_test)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    results[name] = acc\n",
    "\n",
    "sorted_results = sorted(results.items(), key=lambda x: x[1], reverse=True)\n",
    "for name, acc in sorted_results:\n",
    "    print(f\"{name}: {acc:.4f}\")\n",
    "\n",
    "best_model_name, best_acc = sorted_results[0]\n",
    "print(\"\\nBest Model:\", best_model_name, \"| Accuracy:\", best_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4edb3dc3-464e-4071-9b41-e29dbfaf5cbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "best_model = models[best_model_name] \n",
    "\n",
    "review1 = \"this movie was terrible and bad\"\n",
    "review2 = \"i really liked the movie and enjoyed a lot\"\n",
    "\n",
    "x_new = converter.transform([review1, review2]).toarray()\n",
    "\n",
    "predictions = best_model.predict(x_new)\n",
    "\n",
    "\n",
    "print(predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a21e1423-a1c7-4987-b917-484a22c4025c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Save best model\n",
    "with open(\"model.pkl\", \"wb\") as f:\n",
    "    pickle.dump(best_model, f)\n",
    "\n",
    "# Save TF-IDF vectorizer\n",
    "with open(\"vectorizer.pkl\", \"wb\") as f:\n",
    "    pickle.dump(converter, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db4922d-e0f5-4183-86ca-93e60f4a87d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
